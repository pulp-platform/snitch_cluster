#!/usr/bin/env python3
# Copyright 2020 ETH Zurich and University of Bologna.
# Licensed under the Apache License, Version 2.0, see LICENSE for details.
# SPDX-License-Identifier: Apache-2.0
#
# Authors: Paul Scheffler <paulsc@iis.ee.ethz.ch>
#          Luca Colagrande <colluca@iis.ee.ethz.ch>
"""Script to generate human-readable instruction traces for Snitch.

This script takes a trace generated by a Snitch hart
(see `snitch_cc.sv`) and transforms the additional decode stage info
into meaningful annotation.

It also counts and computes various performance metrics for every
execution region. An execution region is a sequence of instructions.
Every `mcycle` CSR read instruction in your trace implicitly defines
two execution regions, comprising respectively:

- all instructions executed before the read, up to the previous read
or the first executed instruction
- all instructions executed after the read, up to the next read or
the last executed instruction

Performance metrics are appended at the end of the generated trace
and can optionally be dumped to a separate JSON file.

It also computes various performance metrics for every DMA transfer,
provided that the Snitch core is equipped with a tightly-coupled DMA
engine, and the DMA trace logged during simulation is fed to the tool.
DMA performance metrics are dumped to a separate JSON file.
"""

# TODO: OPER_TYPES and FPU_OPER_TYPES could break: optimization might alter enum mapping
# TODO: We annotate all FP16 LSU values as IEEE, not FP16ALT... can we do better?

import sys
import re
import argparse
import subprocess
import json
import ast
from ctypes import c_int32, c_uint32
from collections import deque, defaultdict
from pathlib import Path
import traceback
from typing import Optional
from itertools import tee, islice, chain
from functools import lru_cache
from snitch.util.trace.sequencer import Sequencer
import warnings

DASM_IN_REGEX = r'DASM\(([0-9a-fA-F]+)\)'

TRACE_IN_REGEX = r'(\d+)\s+(\d+)\s+(\d+)\s+(0x[0-9A-Fa-fz]+)\s+([^#;]*)(\s*#;\s*(.*))?'

TRACE_OUT_FMT = '{:>8} {:>8} {:>8} {:>10} {:<30}'

# -------------------- Tracer configuration  --------------------

# Below this absolute value: use signed int representation. Above: unsigned 32-bit hex
MAX_SIGNED_INT_LIT = 0xFFFF

# Performance keys which only serve to compute other metrics: omit on printing
PERF_EVAL_KEYS_OMIT = ('start', 'end', 'end_fpss', 'snitch_issues',
                       'snitch_load_latency', 'snitch_fseq_offloads',
                       'fseq_issues', 'fpss_issues', 'fpss_fpu_issues',
                       'fpss_load_latency', 'fpss_fpu_latency')
PERF_EVAL_KEYS_DECIMAL = ('tstart', 'tend', 'cycles')

# -------------------- Architectural constants and enums  --------------------

REG_ABI_NAMES_I = ('zero', 'ra', 'sp', 'gp', 'tp', 't0', 't1', 't2', 's0',
                   's1', *('a{}'.format(i) for i in range(8)),
                   *('s{}'.format(i)
                     for i in range(2, 12)), *('t{}'.format(i)
                                               for i in range(3, 7)))

REG_ABI_NAMES_F = (*('ft{}'.format(i) for i in range(0, 8)), 'fs0', 'fs1',
                   'fa0', 'fa1', *('fa{}'.format(i) for i in range(2, 8)),
                   *('fs{}'.format(i)
                     for i in range(2, 12)), *('ft{}'.format(i)
                                               for i in range(8, 12)))

TRACE_SRCES = {'snitch': 0, 'fpu': 1, 'sequencer': 2}

LS_SIZES = ('Byte', 'Half', 'Word', 'Doub')

OPER_TYPES = {'gpr': 1, 'csr': 8}

FPU_OPER_TYPES = ('NONE', 'acc', 'rs1', 'rs2', 'rs3', 'rs1', 'rd')

FLOAT_FMTS = ((8, 23), (11, 52), (5, 10), (5, 2), (8, 7), (4, 3))

LS_TO_FLOAT = (3, 2, 0, 1)

CSR_NAMES = {
    0xc00: 'cycle',
    0xc01: 'time',
    0xc02: 'instret',
    0xc03: 'hpmcounter3',
    0xc04: 'hpmcounter4',
    0xc05: 'hpmcounter5',
    0xc06: 'hpmcounter6',
    0xc07: 'hpmcounter7',
    0xc08: 'hpmcounter8',
    0xc09: 'hpmcounter9',
    0xc0a: 'hpmcounter10',
    0xc0b: 'hpmcounter11',
    0xc0c: 'hpmcounter12',
    0xc0d: 'hpmcounter13',
    0xc0e: 'hpmcounter14',
    0xc0f: 'hpmcounter15',
    0xc10: 'hpmcounter16',
    0xc11: 'hpmcounter17',
    0xc12: 'hpmcounter18',
    0xc13: 'hpmcounter19',
    0xc14: 'hpmcounter20',
    0xc15: 'hpmcounter21',
    0xc16: 'hpmcounter22',
    0xc17: 'hpmcounter23',
    0xc18: 'hpmcounter24',
    0xc19: 'hpmcounter25',
    0xc1a: 'hpmcounter26',
    0xc1b: 'hpmcounter27',
    0xc1c: 'hpmcounter28',
    0xc1d: 'hpmcounter29',
    0xc1e: 'hpmcounter30',
    0xc1f: 'hpmcounter31',
    0x100: 'sstatus',
    0x104: 'sie',
    0x105: 'stvec',
    0x106: 'scounteren',
    0x140: 'sscratch',
    0x141: 'sepc',
    0x142: 'scause',
    0x143: 'stval',
    0x144: 'sip',
    0x180: 'satp',
    0x200: 'bsstatus',
    0x204: 'bsie',
    0x205: 'bstvec',
    0x240: 'bsscratch',
    0x241: 'bsepc',
    0x242: 'bscause',
    0x243: 'bstval',
    0x244: 'bsip',
    0x280: 'bsatp',
    0xa00: 'hstatus',
    0xa02: 'hedeleg',
    0xa03: 'hideleg',
    0xa80: 'hgatp',
    0x7: 'utvt',
    0x45: 'unxti',
    0x46: 'uintstatus',
    0x48: 'uscratchcsw',
    0x49: 'uscratchcswl',
    0x107: 'stvt',
    0x145: 'snxti',
    0x146: 'sintstatus',
    0x148: 'sscratchcsw',
    0x149: 'sscratchcswl',
    0x307: 'mtvt',
    0x345: 'mnxti',
    0x346: 'mintstatus',
    0x348: 'mscratchcsw',
    0x349: 'mscratchcswl',
    0x300: 'mstatus',
    0x301: 'misa',
    0x302: 'medeleg',
    0x303: 'mideleg',
    0x304: 'mie',
    0x305: 'mtvec',
    0x306: 'mcounteren',
    0x340: 'mscratch',
    0x341: 'mepc',
    0x342: 'mcause',
    0x343: 'mtval',
    0x344: 'mip',
    0x3a0: 'pmpcfg0',
    0x3a1: 'pmpcfg1',
    0x3a2: 'pmpcfg2',
    0x3a3: 'pmpcfg3',
    0x3b0: 'pmpaddr0',
    0x3b1: 'pmpaddr1',
    0x3b2: 'pmpaddr2',
    0x3b3: 'pmpaddr3',
    0x3b4: 'pmpaddr4',
    0x3b5: 'pmpaddr5',
    0x3b6: 'pmpaddr6',
    0x3b7: 'pmpaddr7',
    0x3b8: 'pmpaddr8',
    0x3b9: 'pmpaddr9',
    0x3ba: 'pmpaddr10',
    0x3bb: 'pmpaddr11',
    0x3bc: 'pmpaddr12',
    0x3bd: 'pmpaddr13',
    0x3be: 'pmpaddr14',
    0x3bf: 'pmpaddr15',
    0x7a0: 'tselect',
    0x7a1: 'tdata1',
    0x7a2: 'tdata2',
    0x7a3: 'tdata3',
    0x7b0: 'dcsr',
    0x7b1: 'dpc',
    0x7b2: 'dscratch',
    0xb00: 'mcycle',
    0xb02: 'minstret',
    0xb03: 'mhpmcounter3',
    0xb04: 'mhpmcounter4',
    0xb05: 'mhpmcounter5',
    0xb06: 'mhpmcounter6',
    0xb07: 'mhpmcounter7',
    0xb08: 'mhpmcounter8',
    0xb09: 'mhpmcounter9',
    0xb0a: 'mhpmcounter10',
    0xb0b: 'mhpmcounter11',
    0xb0c: 'mhpmcounter12',
    0xb0d: 'mhpmcounter13',
    0xb0e: 'mhpmcounter14',
    0xb0f: 'mhpmcounter15',
    0xb10: 'mhpmcounter16',
    0xb11: 'mhpmcounter17',
    0xb12: 'mhpmcounter18',
    0xb13: 'mhpmcounter19',
    0xb14: 'mhpmcounter20',
    0xb15: 'mhpmcounter21',
    0xb16: 'mhpmcounter22',
    0xb17: 'mhpmcounter23',
    0xb18: 'mhpmcounter24',
    0xb19: 'mhpmcounter25',
    0xb1a: 'mhpmcounter26',
    0xb1b: 'mhpmcounter27',
    0xb1c: 'mhpmcounter28',
    0xb1d: 'mhpmcounter29',
    0xb1e: 'mhpmcounter30',
    0xb1f: 'mhpmcounter31',
    0x323: 'mhpmevent3',
    0x324: 'mhpmevent4',
    0x325: 'mhpmevent5',
    0x326: 'mhpmevent6',
    0x327: 'mhpmevent7',
    0x328: 'mhpmevent8',
    0x329: 'mhpmevent9',
    0x32a: 'mhpmevent10',
    0x32b: 'mhpmevent11',
    0x32c: 'mhpmevent12',
    0x32d: 'mhpmevent13',
    0x32e: 'mhpmevent14',
    0x32f: 'mhpmevent15',
    0x330: 'mhpmevent16',
    0x331: 'mhpmevent17',
    0x332: 'mhpmevent18',
    0x333: 'mhpmevent19',
    0x334: 'mhpmevent20',
    0x335: 'mhpmevent21',
    0x336: 'mhpmevent22',
    0x337: 'mhpmevent23',
    0x338: 'mhpmevent24',
    0x339: 'mhpmevent25',
    0x33a: 'mhpmevent26',
    0x33b: 'mhpmevent27',
    0x33c: 'mhpmevent28',
    0x33d: 'mhpmevent29',
    0x33e: 'mhpmevent30',
    0x33f: 'mhpmevent31',
    0xf11: 'mvendorid',
    0xf12: 'marchid',
    0xf13: 'mimpid',
    0xf14: 'mhartid',
    0xc80: 'cycleh',
    0xc81: 'timeh',
    0xc82: 'instreth',
    0xc83: 'hpmcounter3h',
    0xc84: 'hpmcounter4h',
    0xc85: 'hpmcounter5h',
    0xc86: 'hpmcounter6h',
    0xc87: 'hpmcounter7h',
    0xc88: 'hpmcounter8h',
    0xc89: 'hpmcounter9h',
    0xc8a: 'hpmcounter10h',
    0xc8b: 'hpmcounter11h',
    0xc8c: 'hpmcounter12h',
    0xc8d: 'hpmcounter13h',
    0xc8e: 'hpmcounter14h',
    0xc8f: 'hpmcounter15h',
    0xc90: 'hpmcounter16h',
    0xc91: 'hpmcounter17h',
    0xc92: 'hpmcounter18h',
    0xc93: 'hpmcounter19h',
    0xc94: 'hpmcounter20h',
    0xc95: 'hpmcounter21h',
    0xc96: 'hpmcounter22h',
    0xc97: 'hpmcounter23h',
    0xc98: 'hpmcounter24h',
    0xc99: 'hpmcounter25h',
    0xc9a: 'hpmcounter26h',
    0xc9b: 'hpmcounter27h',
    0xc9c: 'hpmcounter28h',
    0xc9d: 'hpmcounter29h',
    0xc9e: 'hpmcounter30h',
    0xc9f: 'hpmcounter31h',
    0xb80: 'mcycleh',
    0xb82: 'minstreth',
    0xb83: 'mhpmcounter3h',
    0xb84: 'mhpmcounter4h',
    0xb85: 'mhpmcounter5h',
    0xb86: 'mhpmcounter6h',
    0xb87: 'mhpmcounter7h',
    0xb88: 'mhpmcounter8h',
    0xb89: 'mhpmcounter9h',
    0xb8a: 'mhpmcounter10h',
    0xb8b: 'mhpmcounter11h',
    0xb8c: 'mhpmcounter12h',
    0xb8d: 'mhpmcounter13h',
    0xb8e: 'mhpmcounter14h',
    0xb8f: 'mhpmcounter15h',
    0xb90: 'mhpmcounter16h',
    0xb91: 'mhpmcounter17h',
    0xb92: 'mhpmcounter18h',
    0xb93: 'mhpmcounter19h',
    0xb94: 'mhpmcounter20h',
    0xb95: 'mhpmcounter21h',
    0xb96: 'mhpmcounter22h',
    0xb97: 'mhpmcounter23h',
    0xb98: 'mhpmcounter24h',
    0xb99: 'mhpmcounter25h',
    0xb9a: 'mhpmcounter26h',
    0xb9b: 'mhpmcounter27h',
    0xb9c: 'mhpmcounter28h',
    0xb9d: 'mhpmcounter29h',
    0xb9e: 'mhpmcounter30h',
    0xb9f: 'mhpmcounter31h'
}

PRIV_LVL = {'3': 'M', '1': 'S', '0': 'U'}

# -------------------- FPU helpers  --------------------


_cached_opcodes = None


def load_opcodes():
    global _cached_opcodes
    opcode_file_name = 'opcodes-flt-occamy_CUSTOM.csv'
    opcode_file_path = Path(__file__).parent.absolute() / opcode_file_name

    _cached_opcodes = {}
    with open(opcode_file_path, 'r') as f:
        for line in f:
            fields = line.strip().split(',')
            insn_name = fields[0]
            vec_params = fields[1:5]
            _cached_opcodes[insn_name] = vec_params


@lru_cache
def disasm_inst(hex_inst, mc_exec='llvm-mc', mc_flags='-disassemble -mcpu=snitch'):
    """Disassemble a single RISC-V instruction using llvm-mc."""
    # Reverse the endianness of the hex instruction
    inst_fmt = ' '.join(f'0x{byte:02x}' for byte in bytes.fromhex(hex_inst)[::-1])

    # Use llvm-mc to disassemble the binary instruction
    result = subprocess.run(
        [mc_exec] + mc_flags.split(),
        input=inst_fmt,
        capture_output=True,
        text=True,
        check=True,
    )

    # Extract disassembled instruction from llvm-mc output
    return result.stdout.splitlines()[-1].strip().replace('\t', ' ')


def flt_op_vlen(insn: str, op_type: str) -> int:
    """Get the vector length of a floating-point instruction operand.

    Args:
        insn: Instruction as extracted from the trace line.
        op_type: One of the operand types defined in `FPU_OPER_TYPES`.
    Returns:
        The vector length of the operand, greater than one if SIMD.
    """
    global _cached_opcodes
    if _cached_opcodes is None:
        load_opcodes()

    # Cut the instruction after the first space to get the instruction name
    insn = insn.split(' ')[0]

    # Check if operand is a source or a destination
    is_rd = (op_type == 'rd')

    # Get operand parameters from the instruction
    op_params = _cached_opcodes.get(insn, None)

    # Get vector length of operand
    if op_params is not None and op_params != ([''] * 4):
        vlen = int(op_params[3]) if is_rd else int(op_params[1])
    else:
        vlen = 1
    return vlen


def flt_op_fmt(extras: dict, port: int) -> int:
    """Extracts the floating-point format of an instruction operand.

    Args:
        extras: The dictionary containing the instruction's extra
            information.
        port: The index of the floating-point operand.
    """
    op_sel = extras['op_sel_{}'.format(port)]
    oper_type = FPU_OPER_TYPES[op_sel]
    if extras['is_store']:
        return LS_TO_FLOAT[extras['ls_size']]
    else:
        if oper_type == 'rd':
            return extras['dst_fmt']
        else:
            return extras['src_fmt']


def flt_oper(insn: str, extras: dict, port: int) -> (str, str):
    """Extracts details on the floating-point operand of an instruction.

    Args:
        insn: The current instruction mnemonic.
        extras: The dictionary containing the instruction's extra
            information.
        port: The index of the floating-point operand.
    Returns:
        A tuple containing the operand's register name and a string
        literal representing the floating-point value.
    """
    op_sel = extras['op_sel_{}'.format(port)]
    oper_type = FPU_OPER_TYPES[op_sel]

    # Assign default return values
    reg = oper_type
    lit = None

    # If operand comes from accelerator interface, format as integer.
    if oper_type == 'acc':
        reg = 'ac{}'.format(port + 1)
        lit = int_lit(extras['acc_qdata_{}'.format(port)], extras['int_fmt'])
    # If operand is not unspecified, format as floating-point.
    elif oper_type != 'NONE':
        # Get operand vector length, FP format and integer encoding
        vlen = flt_op_vlen(insn, oper_type)
        fmt = flt_op_fmt(extras, port)
        enc = extras['op_{}'.format(port)]
        # Return register name and floating-point literal
        return REG_ABI_NAMES_F[extras[oper_type]], flt_lit(enc, fmt, vlen=vlen)
    return reg, lit


def flt_decode(val: int, fmt: int) -> float:
    """Interprets the binary encoding of an integer as a FP value.

    Args:
        val: The integer encoding of the FP variable to decode.
        fmt: The floating point number format, as an index into the
            `FLOAT_FMTS` array.
    Returns:
        The floating point value represented by the input integer.
    """
    # get format and bit vector
    w_exp, w_mnt = FLOAT_FMTS[fmt]
    width = 1 + w_exp + w_mnt
    bitstr = '{:064b}'.format(val)[-width:]
    # print(bitstr)
    # Read bit vector slices
    sgn = -1.0 if bitstr[0] == '1' else 1.0
    mnt = int(bitstr[w_exp + 1:], 2)
    exp_unb = int(bitstr[1:w_exp + 1], 2)
    # derive base and exponent
    bse = int('1' + bitstr[w_exp + 1:], 2) / (2**w_mnt)
    exp_bias = -(2**(w_exp - 1) - 1)
    exp = exp_unb + exp_bias
    # case analysis
    if exp_unb == 2**w_exp - 1:
        return sgn * float('inf' if mnt == 0 else 'nan')
    elif exp_unb == 0 and mnt == 0:
        return sgn * 0.0
    elif exp_unb == 0:
        return float(sgn * mnt / (2**w_mnt) * (2**(exp_bias + 1)))
    else:
        return float(sgn * bse * (2**exp))


def flt_fmt(flt: float, width: int = 6) -> str:
    """Formats a floating-point number rounding to a certain decimal precision.

    Args:
        flt: The floating-point number to format.
        width: The number of significant decimal digits to round to.
    Returns:
        The formatted floating-point number as a string.
    """
    fmt = '{:.' + str(width) + '}'
    return fmt.format(flt)


# -------------------- Literal formatting  --------------------


def int_lit(num: int, size: int = 2, as_hex: Optional[bool] = None) -> str:
    width = (8 * int(2**size))
    size_mask = (0x1 << width) - 1
    num = num & size_mask  # num is unsigned
    num_signed = c_int32(c_uint32(num).value).value
    if as_hex is True or abs(num_signed) > MAX_SIGNED_INT_LIT and as_hex is not False:
        return '0x{0:0{1}x}'.format(num, width // 4)
    else:
        return str(num_signed)


def flt_lit(num: int, fmt: int, width: int = 6, vlen: int = 1) -> str:
    """Formats an integer encoding into a floating-point literal.

    Args:
        num: The integer encoding of the floating-point number(s).
        fmt: The floating point number format, as an index into the
            `FLOAT_FMTS` array.
        width: The bitwidth of the floating-point type.
        vlen: The number of floating-point numbers packed in the encoding,
            >1 for SIMD vectors.
    """
    # Divide the binary encoding into individual encodings for each number in the SIMD vector.
    bitwidth = 1 + FLOAT_FMTS[fmt][0] + FLOAT_FMTS[fmt][1]
    vec = [num >> (bitwidth * i) & (2**bitwidth - 1) for i in reversed(range(vlen))]
    # Format each individual float encoding to a string.
    floats = [flt_fmt(flt_decode(val, fmt), width) for val in vec]
    # Represent the encodings as a vector if SIMD.
    if len(floats) > 1:
        return '[{}]'.format(', '.join(floats))
    else:
        return floats[0]


# -------------------- DMA --------------------


# We always assume dma_trans contains at least one incomplete placeholder DMA transaction.
# This incomplete transaction contains default settings. Only upon a DMCPY* instruction
# is the size of the transaction known, completing the transaction. At that point, a new
# incomplete transaction is created, inheriting the configuration settings from the previous
# transaction, which may or may not be overriden before the next DMCPY*.
def update_dma(insn, extras, dma_trans):
    # Extract instruction mnemonic from full instruction decoding (includes operand registers)
    MNEMONIC_REGEX = r'^([\w.]+)\s'
    match = re.match(MNEMONIC_REGEX, insn)
    if match:
        mnemonic = match.group(1)
        # Process DMA instruction
        if mnemonic in ['dmsrc', 'dmdst', 'dmstr']:
            pass
        elif mnemonic == 'dmrep':
            dma_trans[-1]['rep'] = extras['opa']
        elif mnemonic in ['dmcpy', 'dmcpyi']:
            # Create new placeholder transaction to inherit current DMA settings
            dma_trans.append(dma_trans[-1].copy())
            # Set size of the transaction
            dma_trans[-2]['size'] = extras['opa']
            # Override repetition count if the transaction is configured to be 1D
            config = extras['rs2']
            enable_2d = (config & 2) >> 1
            if not enable_2d:
                dma_trans[-2]['rep'] = 1


def eval_dma_metrics(dma_trans, dma_trace):
    dma_trace = Path(dma_trace)
    if dma_trace.exists():
        with open(dma_trace, 'r') as f:
            # Initialize variables
            compl_transfers = []
            outst_transfers = []
            transfer_idx = 0
            exp_bytes = 0
            req_bytes = 0
            bursts_in_transfer = 0
            rec_bursts = 0
            # Iterate lines in DMA trace
            for lineno, (line, nextl) in enumerate(current_and_next(f.readlines())):
                try:
                    dma = ast.literal_eval(line)
                except SyntaxError:
                    message = 'Exception occured while processing '
                    if not nextl:
                        message += 'last line. Did the simulation terminate?'
                    else:
                        message += f'line {lineno}.'
                    print(traceback.format_exc(), file=sys.stderr)
                    print(message, file=sys.stderr)
                time = dma['meta']['time']
                # When the first burst in a transfer is granted, we record a new transfer in
                # the outstanding transfers queue, with the information obtained from the core
                # trace. We record the number of bytes moved by each burst in a transfer, and
                # compare the total to the number of bytes moved by the transfer, to count how
                # many bursts belong to the current DMA transfer (a number which is difficult
                # to pre-compute from the core trace as it depends on address alignments, etc.)
                if dma['backend']['req_valid'] and dma['backend']['req_ready']:
                    if req_bytes == 0:
                        exp_bytes = dma_trans[transfer_idx]['rep'] * \
                                    dma_trans[transfer_idx]['size']
                        outst_transfers.append({'tstart': time,
                                                'bytes': exp_bytes})
                    req_bytes += dma['backend']['req_length']
                    bursts_in_transfer += 1
                    # When the aggregate size of the issued bursts matches the size of the current
                    # transfer, we record the number of bursts in the transfer. This info is later
                    # needed to count responses and determine the end time of the transfer.
                    if req_bytes == exp_bytes:
                        outst_transfers[-1]['bursts'] = bursts_in_transfer
                        # Reset the state for the next transfer.
                        req_bytes = 0
                        bursts_in_transfer = 0
                        transfer_idx += 1
                # Upon a burst completion, we increment the received bursts count.
                if dma['backend']['rsp_valid'] and dma['backend']['rsp_ready']:
                    rec_bursts += 1
                    # When the received bursts count matches the expected bursts for the current
                    # transfer we record the end time of the transfer and promote the transfer
                    # from the outstanding to the completed transfers' queue. The first response
                    # may arrive before the last request is issued. To allow for this condition
                    # we default to -1.
                    if rec_bursts == outst_transfers[0].get('bursts', -1):
                        outst_transfers[0]['tend'] = time
                        compl_transfers.append(outst_transfers.pop(0))
                        # Reset the state for the next transfer.
                        rec_bursts = 0
            # Calculate bandwidth of individual transfers
            for transfer in compl_transfers:
                transfer['cycles'] = transfer['tend'] - transfer['tstart']
                if transfer['bytes'] > 0:
                    transfer['bw'] = transfer['bytes'] / transfer['cycles']
            # Calculate aggregate bandwidth: total number of bytes transferred while any transfer is
            # active (accounts for overlaps between transfers).
            prev_trans_end = 0
            active_cycles = 0
            n_bytes = 0
            for transfer in compl_transfers:
                # Calculate active cycles, without double-counting overlaps
                curr_trans_start, curr_trans_end = transfer['tstart'], transfer['tend']
                if curr_trans_start > prev_trans_end:
                    active_cycles += curr_trans_end - curr_trans_start
                else:
                    active_cycles += curr_trans_end - prev_trans_end
                prev_trans_end = curr_trans_end
                # Calculate total number of bytes
                n_bytes += transfer['bytes']
            dma_metrics = {}
            if active_cycles != 0:
                dma_metrics['aggregate_bw'] = n_bytes / active_cycles
            dma_metrics['transfers'] = compl_transfers
            return dma_metrics


# -------------------- Annotation --------------------


def read_annotations(dict_str: str) -> dict:
    # return literal_eval(dict_str) 	# Could be used, but slow due to universality: needs compiler
    return {
        key: int(val, 16)
        for key, val in re.findall(r"'([^']+)'\s*:\s*([^\s,]+)", dict_str)
    }


def annotate_snitch(extras: dict,
                    sim_time: int,
                    cycle: int,
                    pc: int,
                    gpr_wb_info: dict,
                    perf_metrics: list,
                    annot_fseq_offl: bool = False,
                    int_as_hex: Optional[bool] = None,
                    permissive: bool = False) -> str:
    # Compound annotations in datapath order
    ret = []
    # If Sequencer offload: annotate if desired
    if annot_fseq_offl and extras['fpu_offload']:
        target_name = 'FSEQ' if extras['is_seq_insn'] else 'FPSS'
        ret.append('{} <~~ 0x{:08x}'.format(target_name, pc))
    # If exception, annotate
    if not (extras['stall']) and extras['exception']:
        ret.append('exception')
    # Regular linear datapath operation
    if not (extras['stall'] or extras['fpu_offload']):
        # Operand registers
        if extras['opa_select'] == OPER_TYPES['gpr'] and extras['rs1'] != 0:
            ret.append('{:<3} = {}'.format(REG_ABI_NAMES_I[extras['rs1']],
                                           int_lit(extras['opa'])))
        if extras['opb_select'] == OPER_TYPES['gpr'] and extras['rs2'] != 0:
            ret.append('{:<3} = {}'.format(REG_ABI_NAMES_I[extras['rs2']],
                                           int_lit(extras['opb'])))
        # CSR (always operand b)
        if extras['opb_select'] == OPER_TYPES['csr']:
            csr_addr = extras['csr_addr']
            csr_name = CSR_NAMES[
                csr_addr] if csr_addr in CSR_NAMES else 'csr@{:x}'.format(
                    csr_addr)
            cycles_past = extras['opb']
            if csr_name == 'mcycle':
                perf_metrics[-1]['tend'] = sim_time // 1000
                perf_metrics[-1]['end'] = cycles_past
                perf_metrics.append(defaultdict(int))
                perf_metrics[-1]['tstart'] = sim_time // 1000
                perf_metrics[-1]['start'] = cycles_past + 2
            ret.append('{} = {}'.format(csr_name, int_lit(cycles_past)))
        # Load / Store
        if extras['is_load']:
            perf_metrics[-1]['snitch_loads'] += 1
            if extras['rd'] != 0:
                gpr_wb_info[extras['rd']].appendleft({'cycle': cycle})
                ret.append('{:<3} <~~ {}[{}]'.format(
                    REG_ABI_NAMES_I[extras['rd']], LS_SIZES[extras['ls_size']],
                    int_lit(extras['alu_result'], as_hex=int_as_hex)))
        elif extras['is_store']:
            perf_metrics[-1]['snitch_stores'] += 1
            ret.append('{} ~~> {}[{}]'.format(
                int_lit(extras['gpr_rdata_1']), LS_SIZES[extras['ls_size']],
                int_lit(extras['alu_result'], as_hex=int_as_hex)))
        # Branches: all reg-reg ops
        elif extras['is_branch']:
            ret.append(
                '{}taken'.format('' if extras['alu_result'] else 'not '))
        # Datapath (ALU / Jump Target / Bypass) register writeback
        if extras['write_rd'] and extras['rd'] != 0:
            ret.append('(wrb) {:<3} <-- {}'.format(
                REG_ABI_NAMES_I[extras['rd']], int_lit(extras['writeback'])))
    # Retired loads and accelerator (includes FPU) data: can come back on stall and during other ops
    if extras['retire_load'] and extras['lsu_rd'] != 0:
        try:
            start_time = gpr_wb_info[extras['lsu_rd']].pop()['cycle']
            perf_metrics[-1]['snitch_load_latency'] += cycle - start_time
        except IndexError:
            message = (
                f"In cycle {cycle}, LSU attempts writeback to "
                f"{REG_ABI_NAMES_I[extras['lsu_rd']]}, but none is in flight."
            )
            warnings.warn(message)
        ret.append('(lsu) {:<3} <-- {}'.format(
            REG_ABI_NAMES_I[extras['lsu_rd']],
            int_lit(extras['ld_result_32'])))
    if extras['retire_acc'] and extras['acc_pid'] != 0:
        ret.append('(acc) {:<3} <-- {}'.format(
            REG_ABI_NAMES_I[extras['acc_pid']],
            int_lit(extras['acc_pdata_32'])))
    # Any kind of PC change: Branch, Jump, etc.
    if not extras['stall'] and extras['pc_d'] != pc + 4:
        ret.append('goto {}'.format(int_lit(extras['pc_d'])))
    # Return comma-delimited list
    return ', '.join(ret)


def annotate_fpu(
        extras: dict,
        insn: str,
        cycle: int,
        fpr_wb_info: dict,
        perf_metrics: list,
        # Everything FPU does may have been issued in a previous section
        curr_sec: int = -1,
        int_as_hex: Optional[bool] = None,
        permissive: bool = False) -> str:
    ret = []
    # On issuing of instruction
    if extras['acc_q_hs']:
        # If computation initiated: remember FPU destination format and vector length
        if extras['use_fpu'] and not extras['fpu_in_acc']:
            fpr_wb_info[extras['fpu_in_rd']].appendleft({
                'fmt': extras['dst_fmt'],
                'vlen': flt_op_vlen(insn, 'rd'),
                'cycle': cycle
            })
        # Operands: omit on store
        if not extras['is_store']:
            for i_op in range(3):
                # operand name and its value
                oper_name, val = flt_oper(insn, extras, i_op)
                if oper_name != 'NONE':
                    ret.append('{:<4} = {}'.format(oper_name, val))
        # Load / Store requests
        if extras['lsu_q_hs']:
            s = extras['ls_size']
            if extras['is_load']:
                perf_metrics[curr_sec]['fpss_loads'] += 1
                # Load initiated: remember LSU destination format
                fpr_wb_info[extras['rd']].appendleft({
                    'fmt': LS_TO_FLOAT[s],
                    'vlen': 1,
                    'cycle': cycle
                })
                ret.append('{:<4} <~~ {}[{}]'.format(
                    REG_ABI_NAMES_F[extras['rd']], LS_SIZES[s],
                    int_lit(extras['lsu_qaddr'], as_hex=int_as_hex)))
            if extras['is_store']:
                perf_metrics[curr_sec]['fpss_stores'] += 1
                _, val = flt_oper(insn, extras, 1)
                ret.append('{} ~~> {}[{}]'.format(
                    val, LS_SIZES[s],
                    int_lit(extras['lsu_qaddr'], as_hex=int_as_hex)))
    # On FLOP completion
    if extras['fpu_out_hs']:
        perf_metrics[-1]['fpss_fpu_issues'] += 1
    # Register writeback
    if extras['fpr_we']:
        writer = 'acc' if extras['acc_q_hs'] and extras['acc_wb_ready'] else (
            'fpu'
            if extras['fpu_out_hs'] and not extras['fpu_out_acc'] else 'lsu')
        fmt = 0  # accelerator bus format is 0 for regular float32
        if writer == 'fpu' or writer == 'lsu':
            try:
                fmt, vlen, start_time = fpr_wb_info[extras['fpr_waddr']].pop().values()
                if writer == 'lsu':
                    perf_metrics[curr_sec][
                        'fpss_load_latency'] += cycle - start_time
                else:
                    perf_metrics[curr_sec][
                        'fpss_fpu_latency'] += cycle - start_time
            except IndexError:
                message = (
                    f'In cycle {cycle}, {writer.upper()} attempts writeback to '
                    f'{REG_ABI_NAMES_F[extras["fpr_waddr"]]}, but none in flight.'
                )
                warnings.warn(message)
        ret.append('(f:{}) {:<4} <-- {}'.format(
            writer, REG_ABI_NAMES_F[extras['fpr_waddr']],
            flt_lit(extras['fpr_wdata'], fmt, vlen=vlen)))
    return ', '.join(ret)


# noinspection PyTypeChecker
def annotate_insn(
    line: str,
    gpr_wb_info:
    dict,  # One deque (FIFO) per GPR storing start cycles for each GPR WB
    fpr_wb_info:
    dict,  # One deque (FIFO) per FPR storing start cycles and formats for each FPR WB
    sequencer:
    Sequencer,  # Sequencer model to properly map tunneled instruction PCs
    perf_metrics: list,  # A list performance metric dicts
    mc_exec: str,  # Path to the llvm-mc executable
    mc_flags: str,  # Flags to pass to the llvm-mc executable
    dupl_time_info:
    bool = True,  # Show sim time and cycle again if same as previous line?
    last_time_info:
    tuple = None,  # Previous timestamp (keeps this method stateless)
    annot_fseq_offl:
    bool = False,  # Annotate whenever core offloads to CPU on own line
    int_as_hex: Optional[bool] = None,
    permissive: bool = True,
    dma_trans: list = []
) -> (str, tuple, bool
      ):  # Return time info, whether trace line contains no info, and fseq_len

    # Disassemble instruction
    match = re.search(DASM_IN_REGEX, line)
    if match is not None:
        line = re.sub(
            DASM_IN_REGEX,
            disasm_inst(match.groups()[0], mc_exec, mc_flags),
            line,
        )
    match = re.search(TRACE_IN_REGEX, line.strip('\n'))
    if match is None:
        raise ValueError('Not a valid trace line:\n{}'.format(line))
    time_str, cycle_str, priv_lvl, pc_str, insn, _, extras_str = match.groups()
    time_info = (int(time_str), int(cycle_str))
    show_time_info = (dupl_time_info or time_info != last_time_info)
    time_info_strs = tuple(
        (str(elem) if show_time_info else '') for elem in time_info)
    # Annotated trace
    if extras_str:
        extras = read_annotations(extras_str)
        # Parse lines traced by Snitch
        if extras['source'] == TRACE_SRCES['snitch']:
            annot = annotate_snitch(extras, time_info[0], time_info[1],
                                    int(pc_str, 16), gpr_wb_info, perf_metrics,
                                    annot_fseq_offl, int_as_hex, permissive)
            # Record instructions offloaded from Snitch to the FPSS
            if extras['fpu_offload']:
                perf_metrics[-1]['snitch_fseq_offloads'] += 1
                sequencer.push_insn({
                    "pc": pc_str,
                    "sec": len(perf_metrics) - 1,
                    "is_frep": extras['is_seq_insn']
                })
            if extras['stall'] or extras['fpu_offload']:
                insn, pc_str = ('', '')
            else:
                perf_metrics[-1]['snitch_issues'] += 1
            update_dma(insn, extras, dma_trans)
        # Parse lines traced by the sequencer
        elif extras['source'] == TRACE_SRCES['sequencer']:
            # Sequencer only traces FREP configurations when the respective
            # FREP instruction is handshaked within the sequencer.
            assert (extras['cbuf_push']), 'Unexpected sequencer trace line'
            pc_str, insn, annot = sequencer.decode_frep(extras)
        # Parse lines traced by the FPSS
        elif extras['source'] == TRACE_SRCES['fpu']:
            annot_list = []
            # Parse lines corresponding to instruction issues
            if extras['acc_q_hs']:
                # Emulate one sequencer step, i.e. one instruction issue
                pc_str, curr_sec, frep_iter = sequencer.emulate(permissive)
                # Record cycle in case this was last insn in section
                perf_metrics[curr_sec]['end_fpss'] = time_info[1]
                perf_metrics[curr_sec]['fpss_issues'] += 1
                if frep_iter is not None:
                    # Annotate the current loop iteration
                    frep_iter_annot = f'[{frep_iter["pc"][-4:]}'
                    frep_iter_annot += f' {frep_iter["iter_idx"]}:{frep_iter["inst_idx"]}]'
                    annot_list.append(frep_iter_annot)
            # For all other trace lines (e.g. writebacks) we do not track the
            # respective originating instruction, so we cannot annotate it
            else:
                insn, pc_str = ('', '')
            annot_list.append(
                annotate_fpu(extras, insn, time_info[1], fpr_wb_info, perf_metrics,
                             sequencer.curr_sec, int_as_hex,
                             permissive))
            annot = ', '.join(annot_list)
        else:
            raise ValueError('Unknown trace source: {}'.format(
                extras['source']))
        empty = not (
            insn or annot
        )  # omit empty trace lines (due to double stalls, performance measures)
        if empty:
            # Reset time info if empty: last line on record is previous one!
            time_info = last_time_info
        return (TRACE_OUT_FMT + ' #; {}').format(*time_info_strs,
                                                 PRIV_LVL[priv_lvl], pc_str,
                                                 insn, annot), time_info, empty
    # Vanilla trace
    else:
        return TRACE_OUT_FMT.format(*time_info_strs, PRIV_LVL[priv_lvl],
                                    pc_str, insn), time_info, False


# -------------------- Performance metrics --------------------


def safe_div(dividend, divisor, zero_div=0):
    return dividend / divisor if divisor else zero_div


def eval_perf_metrics(perf_metrics: list):
    for seg in perf_metrics:
        fpss_latency = max(seg['end_fpss'] - seg['end'], 0)
        end = seg[
            'end'] + fpss_latency  # This can be argued over, but it's the most conservatice choice
        cycles = end - seg['start'] + 1
        fpss_fpu_rel_issues = safe_div(seg['fpss_fpu_issues'],
                                       seg['fpss_issues'])
        seg.update({
            # Snitch
            'snitch_avg_load_latency':
            safe_div(seg['snitch_load_latency'], seg['snitch_loads']),
            'snitch_occupancy':
            safe_div(seg['snitch_issues'], cycles),
            'snitch_fseq_rel_offloads':
            safe_div(seg['snitch_fseq_offloads'],
                     seg['snitch_issues'] + seg['snitch_fseq_offloads']),
            # FSeq
            'fseq_yield':
            safe_div(seg['fpss_issues'], seg['snitch_fseq_offloads']),
            'fseq_fpu_yield':
            safe_div(
                safe_div(seg['fpss_fpu_issues'], seg['snitch_fseq_offloads']),
                fpss_fpu_rel_issues),
            # FPSS
            'fpss_section_latency':
            fpss_latency,
            'fpss_avg_fpu_latency':
            safe_div(seg['fpss_fpu_latency'], seg['fpss_fpu_issues']),
            'fpss_avg_load_latency':
            safe_div(seg['fpss_load_latency'], seg['fpss_loads']),
            'fpss_occupancy':
            safe_div(seg['fpss_issues'], cycles),
            'fpss_fpu_occupancy':
            safe_div(seg['fpss_fpu_issues'], cycles),
            'fpss_fpu_rel_occupancy':
            fpss_fpu_rel_issues
        })
        seg['cycles'] = cycles
        seg['total_ipc'] = seg['fpss_occupancy'] + seg['snitch_occupancy']


def fmt_perf_metrics(perf_metrics: list, idx: int, omit_keys: bool = True):
    ret = [
        'Performance metrics for section {} @ ({}, {}):'.format(
            idx, perf_metrics[idx]['start'], perf_metrics[idx]['end'])
    ]
    for key, val in perf_metrics[idx].items():
        if omit_keys and key in PERF_EVAL_KEYS_OMIT:
            continue
        if val is None:
            val_str = str(None)
        elif isinstance(val, float):
            val_str = flt_fmt(val, 4)
        else:
            as_hex = False if key in PERF_EVAL_KEYS_DECIMAL else None
            val_str = int_lit(val, as_hex=as_hex)
        ret.append('{:<40}{:>10}'.format(key, val_str))
    return '\n'.join(ret)


# -------------------- Utils --------------------


def current_and_next(iterable):
    currs, nexts = tee(iterable, 2)
    nexts = chain(islice(nexts, 1, None), [None])
    return zip(currs, nexts)


# -------------------- Main --------------------


# noinspection PyTypeChecker
def main():
    # Argument parsing and iterator creation
    parser = argparse.ArgumentParser()
    parser.add_argument(
        'infile',
        metavar='infile.dasm',
        nargs='?',
        type=argparse.FileType('r'),
        default=sys.stdin,
        help='A matching ASCII signal dump',
    )
    parser.add_argument(
        '-o',
        '--output',
        required=True,
        type=argparse.FileType('w'),
        help='Path to the output file'
    )
    parser.add_argument(
        '--offl',
        action='store_true',
        help='Annotate FPSS and sequencer offloads when they happen in core')
    parser.add_argument(
        '-s',
        '--saddr',
        action='store_true',
        help='Use signed decimal (not unsigned hex) for small addresses')
    parser.add_argument(
        '-a',
        '--allkeys',
        action='store_true',
        help='Include performance metrics measured to compute others')
    parser.add_argument(
        '-p',
        '--permissive',
        action='store_true',
        help='State-related errors are reported as warnings')
    parser.add_argument(
        '--dma-trace',
        help='Path to a DMA trace file'
    )
    parser.add_argument(
        '--dump-hart-perf',
        nargs='?',
        type=argparse.FileType('w'),
        help='Dump hart performance metrics as json text.'
    )
    parser.add_argument(
        '--dump-dma-perf',
        help='Dump DMA performance metrics as json text.'
    )
    parser.add_argument(
        '--mc-exec',
        default='llvm-mc',
        help='Path to the llvm-mc executable'
    )
    parser.add_argument(
        '--mc-flags',
        default='-disassemble -mcpu=snitch',
        help='Flags to pass to the llvm-mc executable'
    )

    args = parser.parse_args()

    # Raise errors on warnings unless disabled on the command-line
    warnings.filterwarnings('default' if args.permissive else 'error', category=UserWarning)

    # Simplify warnings to print only filename, lineno and message
    def custom_formatwarning(message, category, filename, lineno, line=None):
        return f"{filename}:{lineno}: {message}\n"
    warnings.formatwarning = custom_formatwarning

    line_iter = iter(args.infile.readline, b'')

    with args.output as file:
        # Prepare stateful data structures
        time_info = None
        gpr_wb_info = defaultdict(deque)
        fpr_wb_info = defaultdict(deque)
        sequencer = Sequencer()
        dma_trans = [{'rep': 1}]
        perf_metrics = [
            defaultdict(int)
        ]  # all values initially 0, also 'start' time of measurement 0
        perf_metrics[0]['start'] = None
        # Parse input line by line
        for lineno, (line, nextl) in enumerate(current_and_next(line_iter)):
            if line:
                try:
                    ann_insn, time_info, empty = annotate_insn(
                        line,
                        gpr_wb_info,
                        fpr_wb_info,
                        sequencer,
                        perf_metrics,
                        args.mc_exec,
                        args.mc_flags,
                        False,
                        time_info,
                        args.offl,
                        None if args.saddr else True,
                        args.permissive,
                        dma_trans,
                    )
                    if perf_metrics[0]['start'] is None:
                        perf_metrics[0]['tstart'] = time_info[0] // 1000
                        perf_metrics[0]['start'] = time_info[1]
                    if not empty:
                        print(ann_insn, file=file)
                except Exception as e:
                    message = 'Exception occured while processing '
                    if not nextl:
                        message += 'last line. Did the simulation terminate?'
                    else:
                        message += f'line {lineno}.'
                    print(traceback.format_exc(), file=sys.stderr)
                    print(message, file=sys.stderr)
                    if not args.permissive:
                        raise e
            else:
                break  # Nothing more in pipe, EOF
        perf_metrics[-1]['tend'] = time_info[0] // 1000
        perf_metrics[-1]['end'] = time_info[1]
        # Compute metrics
        eval_perf_metrics(perf_metrics)
        # Emit metrics
        print('\n## Performance metrics', file=file)
        for idx in range(len(perf_metrics)):
            print('\n' + fmt_perf_metrics(perf_metrics, idx, not args.allkeys), file=file)
        # Emit DMA metrics
        if args.dma_trace:
            dma_metrics = eval_dma_metrics(dma_trans, args.dma_trace)

    # Dump hart performance metrics to JSON file
    if args.dump_hart_perf:
        with args.dump_hart_perf as file:
            file.write(json.dumps(perf_metrics, indent=4))
    # Dump DMA performance metrics to JSON file
    if args.dump_dma_perf and dma_metrics is not None:
        with open(args.dump_dma_perf, 'w') as file:
            file.write(json.dumps(dma_metrics, indent=4))

    # Check for any loose ends and warn before exiting
    def wb_msg(reg_name, transactions):
        n = len(transactions)
        cycles = ', '.join(sorted(map(lambda x: str(x['cycle']), transactions)))
        return (
            f'Missing {n} writebacks to register {reg_name} from transactions '
            f'initiated at cycles: {cycles}.'
        )
    warn_trip = False
    for fpr, que in fpr_wb_info.items():
        if len(que) != 0:
            warn_trip = True
            warnings.warn(wb_msg(REG_ABI_NAMES_F[fpr], que))
    for gpr, que in gpr_wb_info.items():
        if len(que) != 0:
            warn_trip = True
            warnings.warn(wb_msg(REG_ABI_NAMES_I[gpr], que))
    # Check final state of sequencer is clean
    if sequencer.terminate():
        warn_trip = True
    # Issue an additional general warning if any specific warnings were raised
    if warn_trip:
        message = (
            'Inconsistent final state; performance metrics '
            'may be inaccurate. Is this trace complete?'
        )
        warnings.warn(message)
    return 0


if __name__ == '__main__':
    sys.exit(main())
